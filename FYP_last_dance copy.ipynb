{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4a1d9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "971c2aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-07 16:35:05.060783: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-07 16:35:05.113420: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-07 16:35:05.113474: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-07 16:35:05.113523: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-07 16:35:05.125429: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-07 16:35:06.080639: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, losses, optimizers, Model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2841885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "devices = tf.config.list_physical_devices()\n",
    "print(devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e246f61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80939d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_MCI_Features_file_path = r'./feature_extraction/MCI_func_features.pkl'\n",
    "with open(pickle_MCI_Features_file_path, 'rb') as file:\n",
    "    MCI_funct_features = pickle.load(file)\n",
    "MCI_funct_features.shape\n",
    "MCI_funct_features = np.array(MCI_funct_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c813465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 13456)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MCI_funct_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee3a6fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_MCI_Features_file_path = r'feature_extraction/cat12_MCI_structural_features.pkl'\n",
    "with open(pickle_MCI_Features_file_path, 'rb') as file:\n",
    "    MCI_Struct_features = pickle.load(file)\n",
    "MCI_Struct_features.shape\n",
    "MCI_Struct_features = np.array(MCI_Struct_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9886dee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 169, 205, 169)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MCI_Struct_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53ad34c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "structural_features loaded successfully!\n",
      "Shape of loaded array: (40, 169, 205, 169)\n"
     ]
    }
   ],
   "source": [
    "# Define the same file path\n",
    "file_path = r'feature_extraction/cat12_AD_structural_features.pkl'\n",
    "\n",
    "# Load the NumPy array from the file\n",
    "with open(file_path, \"rb\") as file:  # 'rb' means read binary\n",
    "    AD_struct_features = pickle.load(file)\n",
    "\n",
    "print(\"structural_features loaded successfully!\")\n",
    "print(\"Shape of loaded array:\", AD_struct_features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "959837d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 13456)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle_AD_Features_file_path = r\"feature_extraction/AD_func_features.pkl\"\n",
    "with open(pickle_AD_Features_file_path, 'rb') as file:\n",
    "    AD_funct_features = pickle.load(file)\n",
    "AD_funct_features = np.array(AD_funct_features)\n",
    "AD_funct_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "697e6519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "structural_features loaded successfully!\n",
      "Shape of loaded array: (30, 169, 205, 169)\n"
     ]
    }
   ],
   "source": [
    "# Define the same file path\n",
    "file_path = r'feature_extraction/cat12_CN_structural_features.pkl'\n",
    "\n",
    "# Load the NumPy array from the file\n",
    "with open(file_path, \"rb\") as file:  # 'rb' means read binary\n",
    "    CN_struct_features = pickle.load(file)\n",
    "\n",
    "print(\"structural_features loaded successfully!\")\n",
    "print(\"Shape of loaded array:\", CN_struct_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9217afde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 13456)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle_AD_Features_file_path = r\"feature_extraction/CN_func_features.pkl\"\n",
    "with open(pickle_AD_Features_file_path, 'rb') as file:\n",
    "    CN_funct_features = pickle.load(file)\n",
    "CN_funct_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4526fd16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52, 13456), (52, 169, 205, 169))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MCI_funct_features.shape, MCI_Struct_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed6ff037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40, 13456), (40, 169, 205, 169))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AD_funct_features.shape, AD_struct_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ac8adef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30, 13456), (30, 169, 205, 169))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CN_funct_features.shape, CN_struct_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4e4b395",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data Shapes\n",
    "AD_Struct_features = AD_struct_features\n",
    "# Labels\n",
    "MCI_labels = np.zeros(MCI_funct_features.shape[0])  # Label 0 for MCI\n",
    "AD_labels = np.ones(AD_funct_features.shape[0])    # Label 1 for AD\n",
    "CN_labels = np.array([2]*CN_funct_features.shape[0])\n",
    "\n",
    "# Combine functional and structural features\n",
    "funct_features = np.vstack((MCI_funct_features, AD_funct_features, CN_funct_features))\n",
    "struct_features = np.vstack((MCI_Struct_features, AD_Struct_features, CN_struct_features))\n",
    "labels = np.hstack((MCI_labels, AD_labels, CN_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8cce47cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "struct_features = np.expand_dims(struct_features, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "424a6930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(122, 116, 116, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funct_features = funct_features.reshape(-1, 116, 116)\n",
    "funct_features = np.expand_dims(funct_features, axis=-1)\n",
    "funct_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0af957b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "funct_features = np.random.rand(15, 116, 166, 1)\n",
    "struct_features = np.random.rand(15, 169, 205, 169)\n",
    "labels = np.array([0]*5 + [1]*5 + [2]*5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99402c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "794478e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train_3d, X_test_3d, X_train_2d, X_test_2d, y_train, y_test = train_test_split(\n",
    "    struct_features,\n",
    "    funct_features,\n",
    "    labels,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "498c87b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator labels (real images are labeled as 1)\n",
    "real_labels_train = np.ones((X_train_3d.shape[0], 1))\n",
    "real_labels_test = np.ones((X_test_3d.shape[0], 1))\n",
    "\n",
    "# For simplicity, use real labels for reconstructed images\n",
    "disc_targets_3d_train = real_labels_train\n",
    "disc_targets_2d_train = real_labels_train\n",
    "\n",
    "disc_targets_3d_test = real_labels_test\n",
    "disc_targets_2d_test = real_labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aec97d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "\n",
    "def build_encoder_3d_2d(input_shape):\n",
    "    inputs = Input(shape=input_shape) # 169x205 x169 (input, 169 channels)\n",
    "    x = layers.Conv2D(32, (3, 3), activation='gelu', padding='valid')(inputs) # 166×203 ×32\n",
    "    x = layers.MaxPooling2D((2, 2))(x) # 83×101×83 x32\n",
    "    x = layers.Conv2D(64, (3, 3), activation='gelu', padding='valid')(x) # 81×99 x64\n",
    "    x = layers.MaxPooling2D((2, 2))(x) # 40×49×40 x64\n",
    "    x = layers.Conv2D(128, (3, 3), activation='gelu', padding='valid')(x) # 38×47 x128\n",
    "    x = layers.Flatten()(x)\n",
    "    latent = layers.Dense(256, activation='gelu')(x)\n",
    "    encoder = Model(inputs, latent, name='encoder_3d')\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "555b5c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoder_2d(input_shape):\n",
    "    inputs = Input(shape=input_shape) # 116x116 x1\n",
    "    x = layers.Conv2D(32, (3, 3), activation='gelu', padding='valid')(inputs) # 114x114 x32\n",
    "    x = layers.MaxPooling2D((2, 2))(x) # 57x57 x32\n",
    "    x = layers.Conv2D(64, (3, 3), activation='gelu', padding='valid')(x) # 55x55 x64\n",
    "    x = layers.MaxPooling2D((2, 2))(x) # 27x27 x64\n",
    "    x = layers.Conv2D(128, (3, 3), activation='gelu', padding='valid')(x) # 25x25 x128\n",
    "    x = layers.Flatten()(x)\n",
    "    latent = layers.Dense(256, activation='gelu')(x)\n",
    "    encoder = Model(inputs, latent, name='encoder_2d')\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0084b724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_decoder_3d_2d(latent_dim, output_shape):\n",
    "    # output_shape: (height, width, channels) - Original 2D data shape with high number of channels\n",
    "    height, width, channels = output_shape[0], output_shape[1], output_shape[2]\n",
    "    \n",
    "    latent_inputs = Input(shape=(latent_dim,))\n",
    "    \n",
    "    # Define the dimensions to reshape the latent vector\n",
    "    reshape_height = height // 8\n",
    "    reshape_width = width // 8\n",
    "    reshape_filters = 128  # Number of filters\n",
    "    \n",
    "    x = layers.Dense(reshape_height * reshape_width * reshape_filters, activation='gelu')(latent_inputs)\n",
    "    x = layers.Reshape((reshape_height, reshape_width, reshape_filters))(x)\n",
    "    \n",
    "    # Upsample spatial dimensions\n",
    "    x = layers.Conv2DTranspose(128, (3, 3), strides=2, padding='same', activation='gelu')(x)\n",
    "    x = layers.Conv2DTranspose(64, (3, 3), strides=2, padding='same', activation='gelu')(x)\n",
    "    x = layers.Conv2DTranspose(32, (3, 3), strides=2, padding='same', activation='gelu')(x)\n",
    "    \n",
    "    # Final layer to match the number of channels\n",
    "    x = layers.ZeroPadding2D(padding=((0, 1), (0, 5)))(x)\n",
    "    outputs = layers.Conv2DTranspose(channels, (3, 3), padding='same', activation='sigmoid')(x)\n",
    "    assert outputs.shape[1:] == output_shape, f\"Expected output shape {output_shape} but got {outputs.shape[1:]}\"\n",
    "    decoder = Model(latent_inputs, outputs, name='decoder_3d')\n",
    "    return decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "182bf318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_decoder_2d(latent_dim, output_shape):\n",
    "    latent_inputs = Input(shape=(latent_dim,))\n",
    "    x = layers.Dense((output_shape[0]//4)*(output_shape[1]//4)*64, activation='gelu')(latent_inputs)\n",
    "    x = layers.Reshape((output_shape[0]//4, output_shape[1]//4, 64))(x)\n",
    "    x = layers.Conv2DTranspose(128, (3, 3), strides=2, padding='valid', activation='gelu')(x)\n",
    "    x = layers.Conv2DTranspose(64, (3, 3), strides=2, padding='valid', activation='gelu')(x)\n",
    "    x = layers.Conv2DTranspose(1, (3, 3), padding='valid', activation='sigmoid')(x)\n",
    "    outputs = layers.Cropping2D(cropping=((2, 3), (2, 3)))(x) \n",
    "\n",
    "    assert outputs.shape[1:-1] == output_shape, f\"Output shape {outputs.shape[1:-1]} does not match expected shape {output_shape}\"\n",
    "\n",
    "\n",
    "    decoder = Model(latent_inputs, outputs, name='decoder_2d')\n",
    "    return decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa84d0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator_3d_2d(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = layers.Conv2D(32, (3, 3), activation='gelu')(inputs)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(64, (3, 3), activation='gelu')(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    discriminator = Model(inputs, outputs, name='discriminator_3d')\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b00762d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator_2d(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = layers.Conv2D(32, (3, 3), activation='gelu')(inputs)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(64, (3, 3), activation='gelu')(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    discriminator = Model(inputs, outputs, name='discriminator_2d')\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "947f1e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier(input_dim, num_classes):\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    x = layers.Dense(256, activation='gelu')(inputs)\n",
    "    x = layers.Dense(128, activation='gelu')(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    classifier = Model(inputs, outputs, name='classifier')\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ffb776c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-07 16:35:50.718955: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46093 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:4b:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Input shapes\n",
    "input_shape_3d = (169, 205, 169)\n",
    "input_shape_2d = (116, 116, 1)\n",
    "latent_dim = 256\n",
    "num_classes = 3\n",
    "\n",
    "# Build models\n",
    "encoder_3d = build_encoder_3d_2d(input_shape_3d)\n",
    "encoder_2d = build_encoder_2d(input_shape_2d)\n",
    "decoder_3d = build_decoder_3d_2d(latent_dim, input_shape_3d)\n",
    "decoder_2d = build_decoder_2d(latent_dim, input_shape_2d[:-1])\n",
    "discriminator_3d = build_discriminator_3d_2d(input_shape_3d)\n",
    "discriminator_2d = build_discriminator_2d(input_shape_2d)\n",
    "# Build classifier that accepts concatenated latent vectors\n",
    "classifier = build_classifier(latent_dim * 2, num_classes)\n",
    "\n",
    "# Inputs\n",
    "input_3d = Input(shape=input_shape_3d)\n",
    "input_2d = Input(shape=input_shape_2d)\n",
    "\n",
    "# Encoding\n",
    "latent_3d = encoder_3d(input_3d)\n",
    "latent_2d = encoder_2d(input_2d)\n",
    "\n",
    "# Concatenate the two latent representations\n",
    "combined_latent = layers.Concatenate()([latent_3d, latent_2d])\n",
    "\n",
    "# Decoding\n",
    "reconstructed_3d = decoder_3d(latent_3d)\n",
    "reconstructed_2d = decoder_2d(latent_2d)\n",
    "\n",
    "# Discriminator outputs\n",
    "disc_output_3d = discriminator_3d(reconstructed_3d)\n",
    "disc_output_2d = discriminator_2d(reconstructed_2d)\n",
    "\n",
    "# Get classification output\n",
    "classification_output = classifier(combined_latent)\n",
    "\n",
    "# Define the combined model\n",
    "model = Model(\n",
    "    inputs=[input_3d, input_2d],\n",
    "    outputs=[\n",
    "        reconstructed_3d,\n",
    "        reconstructed_2d,\n",
    "        disc_output_3d,\n",
    "        disc_output_2d,\n",
    "        classification_output\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2fa41a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss weights\n",
    "lambda_reconstruction = 1.0\n",
    "lambda_adversarial = 0.1\n",
    "lambda_classification = 1.0\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={\n",
    "        'decoder_3d': 'mse',\n",
    "        'decoder_2d': 'mse',\n",
    "        'discriminator_3d': tf.keras.losses.BinaryCrossentropy(),\n",
    "        'discriminator_2d': tf.keras.losses.BinaryCrossentropy(),\n",
    "        'classifier': 'sparse_categorical_crossentropy'\n",
    "    },\n",
    "    loss_weights={\n",
    "        'decoder_3d': lambda_reconstruction,\n",
    "        'decoder_2d': lambda_reconstruction,\n",
    "        'discriminator_3d': lambda_adversarial,\n",
    "        'discriminator_2d': lambda_adversarial,\n",
    "        'classifier': lambda_classification\n",
    "    },\n",
    "    metrics={\n",
    "        'classifier': 'accuracy'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f2bceba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator labels (real images are labeled as 1)\n",
    "real_labels = np.ones((struct_features.shape[0], 1))\n",
    "fake_labels = np.zeros((struct_features.shape[0], 1))\n",
    "\n",
    "# For simplicity, use real labels for reconstructed images (you can adjust as needed)\n",
    "disc_targets_3d = real_labels\n",
    "disc_targets_2d = real_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "71fb2286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-07 16:36:13.718299: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8700\n",
      "2024-12-07 16:36:18.174232: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7171544ab040 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-12-07 16:36:18.174302: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2024-12-07 16:36:18.189263: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-12-07 16:36:18.472449: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 23s 598ms/step - loss: 8.5243 - decoder_3d_loss: 0.1918 - decoder_2d_loss: 0.0659 - discriminator_3d_loss: 0.0599 - discriminator_2d_loss: 0.0582 - classifier_loss: 8.2549 - classifier_accuracy: 0.3608 - val_loss: 1.6538 - val_decoder_3d_loss: 0.1889 - val_decoder_2d_loss: 0.0620 - val_discriminator_3d_loss: 0.0000e+00 - val_discriminator_2d_loss: 3.0302e-20 - val_classifier_loss: 1.4028 - val_classifier_accuracy: 0.4800\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 2s 182ms/step - loss: 2.6598 - decoder_3d_loss: 0.1233 - decoder_2d_loss: 0.0604 - discriminator_3d_loss: 2.2360e-31 - discriminator_2d_loss: 2.9681e-21 - classifier_loss: 2.4761 - classifier_accuracy: 0.3918 - val_loss: 3.1271 - val_decoder_3d_loss: 0.0755 - val_decoder_2d_loss: 0.0471 - val_discriminator_3d_loss: 0.0000e+00 - val_discriminator_2d_loss: 1.2084e-25 - val_classifier_loss: 3.0045 - val_classifier_accuracy: 0.4800\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 2s 174ms/step - loss: 2.1083 - decoder_3d_loss: 0.0672 - decoder_2d_loss: 0.0518 - discriminator_3d_loss: 0.0000e+00 - discriminator_2d_loss: 2.8897e-18 - classifier_loss: 1.9894 - classifier_accuracy: 0.3299 - val_loss: 5.5095 - val_decoder_3d_loss: 0.0678 - val_decoder_2d_loss: 0.0449 - val_discriminator_3d_loss: 0.0000e+00 - val_discriminator_2d_loss: 7.4532e-28 - val_classifier_loss: 5.3968 - val_classifier_accuracy: 0.3200\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 2s 180ms/step - loss: 1.7950 - decoder_3d_loss: 0.0643 - decoder_2d_loss: 0.0493 - discriminator_3d_loss: 0.0000e+00 - discriminator_2d_loss: 1.1100e-23 - classifier_loss: 1.6814 - classifier_accuracy: 0.4639 - val_loss: 5.3454 - val_decoder_3d_loss: 0.0470 - val_decoder_2d_loss: 0.0460 - val_discriminator_3d_loss: 0.0000e+00 - val_discriminator_2d_loss: 1.5523e-29 - val_classifier_loss: 5.2523 - val_classifier_accuracy: 0.2000\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 2s 181ms/step - loss: 3.8011 - decoder_3d_loss: 0.0533 - decoder_2d_loss: 0.0461 - discriminator_3d_loss: 0.0000e+00 - discriminator_2d_loss: 3.2365e-14 - classifier_loss: 3.7016 - classifier_accuracy: 0.3196 - val_loss: 1.8882 - val_decoder_3d_loss: 0.0441 - val_decoder_2d_loss: 0.0443 - val_discriminator_3d_loss: 0.0000e+00 - val_discriminator_2d_loss: 8.9927e-21 - val_classifier_loss: 1.7997 - val_classifier_accuracy: 0.3200\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 2s 175ms/step - loss: 1.7197 - decoder_3d_loss: 0.0399 - decoder_2d_loss: 0.0454 - discriminator_3d_loss: 0.0000e+00 - discriminator_2d_loss: 1.3263e-18 - classifier_loss: 1.6345 - classifier_accuracy: 0.4330 - val_loss: 1.6668 - val_decoder_3d_loss: 0.0346 - val_decoder_2d_loss: 0.0444 - val_discriminator_3d_loss: 0.0000e+00 - val_discriminator_2d_loss: 9.7668e-32 - val_classifier_loss: 1.5878 - val_classifier_accuracy: 0.4800\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 2s 169ms/step - loss: 1.2716 - decoder_3d_loss: 0.0318 - decoder_2d_loss: 0.0380 - discriminator_3d_loss: 0.0000e+00 - discriminator_2d_loss: 1.3876e-16 - classifier_loss: 1.2018 - classifier_accuracy: 0.5155 - val_loss: 2.4420 - val_decoder_3d_loss: 0.0293 - val_decoder_2d_loss: 0.0359 - val_discriminator_3d_loss: 0.0000e+00 - val_discriminator_2d_loss: 4.9412e-22 - val_classifier_loss: 2.3767 - val_classifier_accuracy: 0.2800\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 3s 252ms/step - loss: 1.3141 - decoder_3d_loss: 0.0296 - decoder_2d_loss: 0.0322 - discriminator_3d_loss: 0.0000e+00 - discriminator_2d_loss: 1.3983e-17 - classifier_loss: 1.2523 - classifier_accuracy: 0.5052 - val_loss: 1.4731 - val_decoder_3d_loss: 0.0278 - val_decoder_2d_loss: 0.0332 - val_discriminator_3d_loss: 0.0000e+00 - val_discriminator_2d_loss: 2.2289e-24 - val_classifier_loss: 1.4121 - val_classifier_accuracy: 0.3600\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 5s 378ms/step - loss: 1.2549 - decoder_3d_loss: 0.0277 - decoder_2d_loss: 0.0310 - discriminator_3d_loss: 0.0000e+00 - discriminator_2d_loss: 5.2916e-17 - classifier_loss: 1.1961 - classifier_accuracy: 0.5670 - val_loss: 1.1601 - val_decoder_3d_loss: 0.0267 - val_decoder_2d_loss: 0.0406 - val_discriminator_3d_loss: 0.0000e+00 - val_discriminator_2d_loss: 7.8405e-22 - val_classifier_loss: 1.0928 - val_classifier_accuracy: 0.5600\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 5s 388ms/step - loss: 0.7208 - decoder_3d_loss: 0.0259 - decoder_2d_loss: 0.0361 - discriminator_3d_loss: 0.0000e+00 - discriminator_2d_loss: 4.4952e-22 - classifier_loss: 0.6587 - classifier_accuracy: 0.7113 - val_loss: 1.7414 - val_decoder_3d_loss: 0.0254 - val_decoder_2d_loss: 0.0310 - val_discriminator_3d_loss: 0.0000e+00 - val_discriminator_2d_loss: 2.5648e-25 - val_classifier_loss: 1.6850 - val_classifier_accuracy: 0.4400\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 3s 230ms/step - loss: 0.4310 - decoder_3d_loss: 0.0250 - decoder_2d_loss: 0.0306 - discriminator_3d_loss: 0.0000e+00 - discriminator_2d_loss: 1.9799e-14 - classifier_loss: 0.3753 - classifier_accuracy: 0.8041 - val_loss: 1.9089 - val_decoder_3d_loss: 0.0247 - val_decoder_2d_loss: 0.0355 - val_discriminator_3d_loss: 0.0000e+00 - val_discriminator_2d_loss: 9.9298e-29 - val_classifier_loss: 1.8487 - val_classifier_accuracy: 0.5200\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 4s 325ms/step - loss: 0.2580 - decoder_3d_loss: 0.0245 - decoder_2d_loss: 0.0300 - discriminator_3d_loss: 0.0000e+00 - discriminator_2d_loss: 2.5375e-17 - classifier_loss: 0.2034 - classifier_accuracy: 0.9278 - val_loss: 1.8985 - val_decoder_3d_loss: 0.0243 - val_decoder_2d_loss: 0.0330 - val_discriminator_3d_loss: 0.0000e+00 - val_discriminator_2d_loss: 2.1622e-27 - val_classifier_loss: 1.8412 - val_classifier_accuracy: 0.5200\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 5s 386ms/step - loss: 0.2525 - decoder_3d_loss: 0.0241 - decoder_2d_loss: 0.0298 - discriminator_3d_loss: 0.0000e+00 - discriminator_2d_loss: 9.6709e-16 - classifier_loss: 0.1985 - classifier_accuracy: 0.9485 - val_loss: 1.8759 - val_decoder_3d_loss: 0.0238 - val_decoder_2d_loss: 0.0302 - val_discriminator_3d_loss: 0.0000e+00 - val_discriminator_2d_loss: 1.3009e-22 - val_classifier_loss: 1.8219 - val_classifier_accuracy: 0.5200\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 3s 204ms/step - loss: 0.1728 - decoder_3d_loss: 0.0237 - decoder_2d_loss: 0.0287 - discriminator_3d_loss: 0.0000e+00 - discriminator_2d_loss: 4.6900e-19 - classifier_loss: 0.1204 - classifier_accuracy: 0.9794 - val_loss: 1.5838 - val_decoder_3d_loss: 0.0235 - val_decoder_2d_loss: 0.0303 - val_discriminator_3d_loss: 0.0000e+00 - val_discriminator_2d_loss: 1.5808e-18 - val_classifier_loss: 1.5300 - val_classifier_accuracy: 0.6800\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 2s 173ms/step - loss: 0.1775 - decoder_3d_loss: 0.0234 - decoder_2d_loss: 0.0272 - discriminator_3d_loss: 0.0000e+00 - discriminator_2d_loss: 2.0010e-14 - classifier_loss: 0.1270 - classifier_accuracy: 0.9278 - val_loss: 3.4116 - val_decoder_3d_loss: 0.0232 - val_decoder_2d_loss: 0.0297 - val_discriminator_3d_loss: 0.0000e+00 - val_discriminator_2d_loss: 1.0099e-22 - val_classifier_loss: 3.3586 - val_classifier_accuracy: 0.5200\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 3s 272ms/step - loss: 0.0817 - decoder_3d_loss: 0.0230 - decoder_2d_loss: 0.0246 - discriminator_3d_loss: 0.0000e+00 - discriminator_2d_loss: 1.5471e-15 - classifier_loss: 0.0341 - classifier_accuracy: 0.9897 - val_loss: 2.0222 - val_decoder_3d_loss: 0.0227 - val_decoder_2d_loss: 0.0285 - val_discriminator_3d_loss: 0.0000e+00 - val_discriminator_2d_loss: 9.1476e-24 - val_classifier_loss: 1.9710 - val_classifier_accuracy: 0.6000\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 3s 218ms/step - loss: 0.0618 - decoder_3d_loss: 0.0225 - decoder_2d_loss: 0.0233 - discriminator_3d_loss: 0.0000e+00 - discriminator_2d_loss: 2.5675e-18 - classifier_loss: 0.0159 - classifier_accuracy: 1.0000 - val_loss: 2.9827 - val_decoder_3d_loss: 0.0222 - val_decoder_2d_loss: 0.0276 - val_discriminator_3d_loss: 0.0000e+00 - val_discriminator_2d_loss: 5.8967e-23 - val_classifier_loss: 2.9328 - val_classifier_accuracy: 0.5600\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 2s 176ms/step - loss: 0.0575 - decoder_3d_loss: 0.0216 - decoder_2d_loss: 0.0222 - discriminator_3d_loss: 0.0000e+00 - discriminator_2d_loss: 1.0774e-17 - classifier_loss: 0.0136 - classifier_accuracy: 0.9897 - val_loss: 2.4287 - val_decoder_3d_loss: 0.0206 - val_decoder_2d_loss: 0.0274 - val_discriminator_3d_loss: 0.0000e+00 - val_discriminator_2d_loss: 1.2426e-23 - val_classifier_loss: 2.3807 - val_classifier_accuracy: 0.6000\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 2s 176ms/step - loss: 0.1192 - decoder_3d_loss: 0.0198 - decoder_2d_loss: 0.0213 - discriminator_3d_loss: 0.0000e+00 - discriminator_2d_loss: 5.2880e-17 - classifier_loss: 0.0780 - classifier_accuracy: 0.9897 - val_loss: 3.0681 - val_decoder_3d_loss: 0.0188 - val_decoder_2d_loss: 0.0268 - val_discriminator_3d_loss: 0.0000e+00 - val_discriminator_2d_loss: 5.7187e-25 - val_classifier_loss: 3.0225 - val_classifier_accuracy: 0.5200\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 2s 176ms/step - loss: 0.0805 - decoder_3d_loss: 0.0183 - decoder_2d_loss: 0.0206 - discriminator_3d_loss: 0.0000e+00 - discriminator_2d_loss: 7.2448e-19 - classifier_loss: 0.0416 - classifier_accuracy: 0.9897 - val_loss: 2.2207 - val_decoder_3d_loss: 0.0176 - val_decoder_2d_loss: 0.0271 - val_discriminator_3d_loss: 0.0000e+00 - val_discriminator_2d_loss: 1.9823e-22 - val_classifier_loss: 2.1759 - val_classifier_accuracy: 0.5600\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    [X_train_3d, X_train_2d],\n",
    "    {\n",
    "        'decoder_3d': X_train_3d,\n",
    "        'decoder_2d': X_train_2d,\n",
    "        'discriminator_3d': disc_targets_3d_train,\n",
    "        'discriminator_2d': disc_targets_2d_train,\n",
    "        'classifier': y_train\n",
    "    },\n",
    "    epochs=20,\n",
    "    batch_size=8,\n",
    "    validation_data=(\n",
    "        [X_test_3d, X_test_2d],\n",
    "        {\n",
    "            'decoder_3d': X_test_3d,\n",
    "            'decoder_2d': X_test_2d,\n",
    "            'discriminator_3d': disc_targets_3d_test,\n",
    "            'discriminator_2d': disc_targets_2d_test,\n",
    "            'classifier': y_test\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a84aa083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Test Accuracy: 56.00%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the classifier accuracy on the test set\n",
    "evaluation = model.evaluate(\n",
    "    [X_test_3d, X_test_2d],\n",
    "    {\n",
    "        'decoder_3d': X_test_3d,\n",
    "        'decoder_2d': X_test_2d,\n",
    "        'discriminator_3d': disc_targets_3d_test,\n",
    "        'discriminator_2d': disc_targets_2d_test,\n",
    "        'classifier': y_test\n",
    "    },\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Get the index of the classification loss and accuracy (depends on the order of outputs)\n",
    "classification_accuracy = evaluation[model.metrics_names.index('classifier_accuracy')]\n",
    "\n",
    "print(f'Classifier Test Accuracy: {classification_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1bf5dcce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Train Accuracy : 100.00\n"
     ]
    }
   ],
   "source": [
    "evaluation_train = model.evaluate(\n",
    "    [X_train_3d, X_train_2d],\n",
    "    {\n",
    "        'decoder_3d': X_train_3d,\n",
    "        'decoder_2d': X_train_2d,\n",
    "        'discriminator_3d': disc_targets_3d_train,\n",
    "        'discriminator_2d': disc_targets_2d_train,\n",
    "        'classifier': y_train\n",
    "    },\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "classification_accuracy = evaluation_train[model.metrics_names.index('classifier_accuracy')]\n",
    "\n",
    "print(f'Classifier Train Accuracy : {classification_accuracy * 100:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "73cd664e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tripti/miniconda3/lib/python3.9/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] Can't close file (file write failed: time = Thu Nov 28 18:09:47 2024\n, filename = 'models/fusion_model_1.h5', file descriptor = 84, errno = 28, error message = 'No space left on device', buf = 0xbfd3120, total write size = 47824, bytes this sub-write = 47824, bytes actually written = 18446744073709551615, offset = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/fusion_model_1.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/h5py/_hl/files.py:581\u001b[0m, in \u001b[0;36mFile.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid\u001b[38;5;241m.\u001b[39mvalid:\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;66;03m# We have to explicitly murder all open objects related to the file\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \n\u001b[1;32m    578\u001b[0m     \u001b[38;5;66;03m# Close file-resident objects first, then the files.\u001b[39;00m\n\u001b[1;32m    579\u001b[0m     \u001b[38;5;66;03m# Otherwise we get errors in MPI mode.\u001b[39;00m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid\u001b[38;5;241m.\u001b[39m_close_open_objects(h5f\u001b[38;5;241m.\u001b[39mOBJ_LOCAL \u001b[38;5;241m|\u001b[39m \u001b[38;5;241m~\u001b[39mh5f\u001b[38;5;241m.\u001b[39mOBJ_FILE)\n\u001b[0;32m--> 581\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid\u001b[38;5;241m.\u001b[39m_close_open_objects(h5f\u001b[38;5;241m.\u001b[39mOBJ_LOCAL \u001b[38;5;241m|\u001b[39m h5f\u001b[38;5;241m.\u001b[39mOBJ_FILE)\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    584\u001b[0m     _objects\u001b[38;5;241m.\u001b[39mnonlocal_close()\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:355\u001b[0m, in \u001b[0;36mh5py.h5f.FileID._close_open_objects\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] Can't close file (file write failed: time = Thu Nov 28 18:09:47 2024\n, filename = 'models/fusion_model_1.h5', file descriptor = 84, errno = 28, error message = 'No space left on device', buf = 0xbfd3120, total write size = 47824, bytes this sub-write = 47824, bytes actually written = 18446744073709551615, offset = 0)"
     ]
    }
   ],
   "source": [
    "model.save(\"models/fusion_model_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "603237ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((97, 169, 205, 169), (25, 116, 116, 1))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_3d.shape, X_test_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fad11b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
