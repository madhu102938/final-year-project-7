{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4a1d9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "971c2aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-29 17:15:10.642835: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-29 17:15:10.693800: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-29 17:15:10.693852: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-29 17:15:10.693891: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-29 17:15:10.705442: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-29 17:15:11.632031: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, losses, optimizers, Model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80939d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_MCI_Features_file_path = r'./feature_extraction/MCI_final_features_all_1.pkl'\n",
    "with open(pickle_MCI_Features_file_path, 'rb') as file:\n",
    "    MCI_funct_features = pickle.load(file)\n",
    "MCI_funct_features.shape\n",
    "MCI_funct_features = np.array(MCI_funct_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c813465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53, 13456)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MCI_funct_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee3a6fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_MCI_Features_file_path = r'feature_extraction/cat12_structural_features.pkl'\n",
    "with open(pickle_MCI_Features_file_path, 'rb') as file:\n",
    "    MCI_Struct_features = pickle.load(file)\n",
    "MCI_Struct_features.shape\n",
    "MCI_Struct_features = np.array(MCI_Struct_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9886dee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53, 169, 205, 169)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MCI_Struct_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53ad34c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "structural_features loaded successfully!\n",
      "Shape of loaded array: (40, 169, 205, 169)\n"
     ]
    }
   ],
   "source": [
    "# Define the same file path\n",
    "file_path = r'feature_extraction/Structal_cat_fpickle.pkl'\n",
    "\n",
    "# Load the NumPy array from the file\n",
    "with open(file_path, \"rb\") as file:  # 'rb' means read binary\n",
    "    loaded_structural_features = pickle.load(file)\n",
    "\n",
    "print(\"structural_features loaded successfully!\")\n",
    "print(\"Shape of loaded array:\", loaded_structural_features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "959837d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 13456)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle_AD_Features_file_path = r\"feature_extraction/func_features_AD.pkl\"\n",
    "with open(pickle_AD_Features_file_path, 'rb') as file:\n",
    "    AD_funct_features = pickle.load(file)\n",
    "AD_funct_features = np.array(AD_funct_features)\n",
    "AD_funct_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "697e6519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53, 13456)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MCI_funct_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9217afde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53, 169, 205, 169)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MCI_Struct_features.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4526fd16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 13456)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AD_funct_features.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed6ff037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 169, 205, 169)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_structural_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4e4b395",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data Shapes\n",
    "AD_Struct_features = loaded_structural_features\n",
    "# Labels\n",
    "MCI_labels = np.zeros(53)  # Label 0 for MCI\n",
    "AD_labels = np.ones(40)    # Label 1 for AD\n",
    "\n",
    "# Combine functional and structural features\n",
    "funct_features = np.vstack((MCI_funct_features, AD_funct_features))\n",
    "struct_features = np.vstack((MCI_Struct_features, AD_Struct_features))\n",
    "labels = np.hstack((MCI_labels, AD_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "114b1a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming you have loaded your SMRI data into variables\n",
    "# MCI_Struct_features and AD_Struct_features\n",
    "\n",
    "# Combine MCI and AD structural features\n",
    "SMRI_data = np.concatenate([MCI_Struct_features, AD_Struct_features], axis=0)\n",
    "SMRI_labels = np.concatenate([\n",
    "    np.zeros(len(MCI_Struct_features)),  # Label 0 for MCI\n",
    "    np.ones(len(AD_Struct_features))     # Label 1 for AD\n",
    "], axis=0)\n",
    "\n",
    "# Add a channel dimension\n",
    "SMRI_data = np.expand_dims(SMRI_data, axis=-1)  # Shape becomes (num_samples, 169, 205, 169, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43d2344e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have loaded your fMRI data into variables\n",
    "# MCI_funct_features and AD_funct_features\n",
    "\n",
    "# Combine MCI and AD functional features\n",
    "fMRI_data = np.concatenate([MCI_funct_features, AD_funct_features], axis=0)\n",
    "\n",
    "# Reshape and add channel dimension\n",
    "fMRI_data = fMRI_data.reshape(-1, 116, 116)\n",
    "fMRI_data = np.expand_dims(fMRI_data, axis=-1)  # Shape becomes (num_samples, 116, 116, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9a4f1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.concatenate([\n",
    "    np.zeros(len(MCI_funct_features)),  # Label 0 for MCI\n",
    "    np.ones(len(AD_funct_features))     # Label 1 for AD\n",
    "], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99402c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "794478e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train_3d, X_test_3d, X_train_2d, X_test_2d, y_train, y_test = train_test_split(\n",
    "    SMRI_data,\n",
    "    fMRI_data,\n",
    "    labels,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "498c87b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator labels (real images are labeled as 1)\n",
    "real_labels_train = np.ones((X_train_3d.shape[0], 1))\n",
    "real_labels_test = np.ones((X_test_3d.shape[0], 1))\n",
    "\n",
    "# For simplicity, use real labels for reconstructed images\n",
    "disc_targets_3d_train = real_labels_train\n",
    "disc_targets_2d_train = real_labels_train\n",
    "\n",
    "disc_targets_3d_test = real_labels_test\n",
    "disc_targets_2d_test = real_labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aec97d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "\n",
    "def build_encoder_3d(input_shape):\n",
    "    inputs = Input(shape=input_shape) # 169x205x169x1 (input, 1 channel)\n",
    "    x = layers.Conv3D(32, (3, 3, 3), activation='gelu', padding='valid')(inputs) # 166×203×166 x32\n",
    "    x = layers.MaxPooling3D((2, 2, 2))(x) # 83×101×83 x32\n",
    "    x = layers.Conv3D(64, (3, 3, 3), activation='gelu', padding='valid')(x) # 81×99×81 x64\n",
    "    x = layers.MaxPooling3D((2, 2, 2))(x) # 40×49×40 x64\n",
    "    x = layers.Conv3D(128, (3, 3, 3), activation='gelu', padding='valid')(x) # 38×47×38 128\n",
    "    x = layers.Flatten()(x)\n",
    "    latent = layers.Dense(256, activation='gelu')(x)\n",
    "    encoder = Model(inputs, latent, name='encoder_3d')\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "555b5c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoder_2d(input_shape):\n",
    "    inputs = Input(shape=input_shape) # 116x116 x1\n",
    "    x = layers.Conv2D(32, (3, 3), activation='gelu', padding='valid')(inputs) # 114x114 x32\n",
    "    x = layers.MaxPooling2D((2, 2))(x) # 57x57 x32\n",
    "    x = layers.Conv2D(64, (3, 3), activation='gelu', padding='valid')(x) # 55x55 x64\n",
    "    x = layers.MaxPooling2D((2, 2))(x) # 27x27 x64\n",
    "    x = layers.Conv2D(128, (3, 3), activation='gelu', padding='valid')(x) # 25x25 x128\n",
    "    x = layers.Flatten()(x)\n",
    "    latent = layers.Dense(256, activation='gelu')(x)\n",
    "    encoder = Model(inputs, latent, name='encoder_2d')\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0084b724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_decoder_3d(latent_dim, output_shape):\n",
    "    latent_inputs = Input(shape=(latent_dim,))\n",
    "    reshape_shape = np.array([output_shape[0]//8, output_shape[1]//8, output_shape[2]//8, 128])\n",
    "    x = layers.Dense(np.prod(reshape_shape), activation='gelu')(latent_inputs)\n",
    "    x = layers.Reshape(reshape_shape)(x)\n",
    "    x = layers.Conv3DTranspose(128, (3, 3, 3), strides=2, padding='valid', activation='gelu')(x)\n",
    "    x = layers.Conv3DTranspose(64, (3, 3, 3), strides=2, padding='valid', activation='gelu')(x)\n",
    "    x = layers.Conv3DTranspose(32, (3, 3, 3), strides=2, padding='valid', activation='gelu')(x)\n",
    "    # Adjust the final Conv3DTranspose layer to match the expected output shape\n",
    "    x = layers.Conv3DTranspose(1, (3, 3, 3), strides=(1, 1, 1), padding='valid', activation='sigmoid')(x)\n",
    "    # x = layers.ZeroPadding3D(padding=((0, 1), (0, 5), (0, 1)))(x)  # Add padding to match (169, 205, 169)    \n",
    "    x = layers.Cropping3D(cropping=((4, 4), (2, 2), (4, 4)))(x)\n",
    "    outputs = x\n",
    "    \n",
    "    # Ensure the output shape matches the input shape of the discriminator\n",
    "    assert outputs.shape[1:] == output_shape, f\"Output shape {outputs.shape[1:]} does not match expected shape {output_shape}\"\n",
    "    \n",
    "    decoder = Model(latent_inputs, outputs, name='decoder_3d')\n",
    "    return decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "182bf318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_decoder_2d(latent_dim, output_shape):\n",
    "    latent_inputs = Input(shape=(latent_dim,))\n",
    "    x = layers.Dense((output_shape[0]//4)*(output_shape[1]//4)*64, activation='gelu')(latent_inputs)\n",
    "    x = layers.Reshape((output_shape[0]//4, output_shape[1]//4, 64))(x)\n",
    "    x = layers.Conv2DTranspose(128, (3, 3), strides=2, padding='valid', activation='gelu')(x)\n",
    "    x = layers.Conv2DTranspose(64, (3, 3), strides=2, padding='valid', activation='gelu')(x)\n",
    "    x = layers.Conv2DTranspose(1, (3, 3), padding='valid', activation='sigmoid')(x)\n",
    "    outputs = layers.Cropping2D(cropping=((2, 3), (2, 3)))(x) \n",
    "\n",
    "    assert outputs.shape[1:-1] == output_shape, f\"Output shape {outputs.shape[1:-1]} does not match expected shape {output_shape}\"\n",
    "\n",
    "\n",
    "    decoder = Model(latent_inputs, outputs, name='decoder_2d')\n",
    "    return decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa84d0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator_3d(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = layers.Conv3D(32, (3, 3, 3), activation='gelu')(inputs)\n",
    "    x = layers.MaxPooling3D((2, 2, 2))(x)\n",
    "    x = layers.Conv3D(64, (3, 3, 3), activation='gelu')(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    discriminator = Model(inputs, outputs, name='discriminator_3d')\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b00762d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator_2d(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = layers.Conv2D(32, (3, 3), activation='gelu')(inputs)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(64, (3, 3), activation='gelu')(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    discriminator = Model(inputs, outputs, name='discriminator_2d')\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "947f1e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier(input_dim, num_classes):\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    x = layers.Dense(256, activation='gelu')(inputs)\n",
    "    x = layers.Dense(128, activation='gelu')(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    classifier = Model(inputs, outputs, name='classifier')\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ffb776c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input shapes\n",
    "input_shape_3d = (169, 205, 169, 1)\n",
    "input_shape_2d = (116, 116, 1)\n",
    "latent_dim = 256\n",
    "num_classes = 2\n",
    "\n",
    "# Build models\n",
    "encoder_3d = build_encoder_3d(input_shape_3d)\n",
    "encoder_2d = build_encoder_2d(input_shape_2d)\n",
    "decoder_3d = build_decoder_3d(latent_dim, input_shape_3d)\n",
    "decoder_2d = build_decoder_2d(latent_dim, input_shape_2d[:-1])\n",
    "discriminator_3d = build_discriminator_3d(input_shape_3d)\n",
    "discriminator_2d = build_discriminator_2d(input_shape_2d)\n",
    "# Build classifier that accepts concatenated latent vectors\n",
    "classifier = build_classifier(latent_dim * 2, num_classes)\n",
    "\n",
    "# Inputs\n",
    "input_3d = Input(shape=input_shape_3d)\n",
    "input_2d = Input(shape=input_shape_2d)\n",
    "\n",
    "# Encoding\n",
    "latent_3d = encoder_3d(input_3d)\n",
    "latent_2d = encoder_2d(input_2d)\n",
    "\n",
    "# Concatenate the two latent representations\n",
    "combined_latent = layers.Concatenate()([latent_3d, latent_2d])\n",
    "\n",
    "# Decoding\n",
    "reconstructed_3d = decoder_3d(latent_3d)\n",
    "reconstructed_2d = decoder_2d(latent_2d)\n",
    "\n",
    "# Discriminator outputs\n",
    "disc_output_3d = discriminator_3d(reconstructed_3d)\n",
    "disc_output_2d = discriminator_2d(reconstructed_2d)\n",
    "\n",
    "# Get classification output\n",
    "classification_output = classifier(combined_latent)\n",
    "\n",
    "# Define the combined model\n",
    "model = Model(\n",
    "    inputs=[input_3d, input_2d],\n",
    "    outputs=[\n",
    "        reconstructed_3d,\n",
    "        reconstructed_2d,\n",
    "        disc_output_3d,\n",
    "        disc_output_2d,\n",
    "        classification_output\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2fa41a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss weights\n",
    "lambda_reconstruction = 1.0\n",
    "lambda_adversarial = 0.1\n",
    "lambda_classification = 1.0\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={\n",
    "        'decoder_3d': 'mse',\n",
    "        'decoder_2d': 'mse',\n",
    "        'discriminator_3d': tf.keras.losses.BinaryCrossentropy(),\n",
    "        'discriminator_2d': tf.keras.losses.BinaryCrossentropy(),\n",
    "        'classifier': 'sparse_categorical_crossentropy'\n",
    "    },\n",
    "    loss_weights={\n",
    "        'decoder_3d': lambda_reconstruction,\n",
    "        'decoder_2d': lambda_reconstruction,\n",
    "        'discriminator_3d': lambda_adversarial,\n",
    "        'discriminator_2d': lambda_adversarial,\n",
    "        'classifier': lambda_classification\n",
    "    },\n",
    "    metrics={\n",
    "        'classifier': 'accuracy'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f2bceba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator labels (real images are labeled as 1)\n",
    "real_labels = np.ones((SMRI_data.shape[0], 1))\n",
    "fake_labels = np.zeros((SMRI_data.shape[0], 1))\n",
    "\n",
    "# For simplicity, use real labels for reconstructed images (you can adjust as needed)\n",
    "disc_targets_3d = real_labels\n",
    "disc_targets_2d = real_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fb2286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-29 17:16:28.129110: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x703e780a5fc0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2024-11-29 17:16:28.129203: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2024-11-29 17:16:28.196251: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-11-29 17:16:29.506232: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2024-11-29 17:16:29.513654: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2024-11-29 17:17:11.973926: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2024-11-29 17:17:12.019663: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/10 [==>...........................] - ETA: 11:03 - loss: 1.0900 - decoder_3d_loss: 0.1957 - decoder_2d_loss: 0.0669 - discriminator_3d_loss: 0.6710 - discriminator_2d_loss: 0.6723 - classifier_loss: 0.6932 - classifier_accuracy: 0.5000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-29 17:17:38.563460: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4/10 [===========>..................] - ETA: 1:54 - loss: nan - decoder_3d_loss: nan - decoder_2d_loss: nan - discriminator_3d_loss: nan - discriminator_2d_loss: nan - classifier_loss: nan - classifier_accuracy: 0.5000                 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-29 17:18:35.585916: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5/10 [==============>...............] - ETA: 1:34 - loss: nan - decoder_3d_loss: nan - decoder_2d_loss: nan - discriminator_3d_loss: nan - discriminator_2d_loss: nan - classifier_loss: nan - classifier_accuracy: 0.5000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-29 17:18:53.352648: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 261s 21s/step - loss: nan - decoder_3d_loss: nan - decoder_2d_loss: nan - discriminator_3d_loss: nan - discriminator_2d_loss: nan - classifier_loss: nan - classifier_accuracy: 0.4865 - val_loss: nan - val_decoder_3d_loss: nan - val_decoder_2d_loss: nan - val_discriminator_3d_loss: nan - val_discriminator_2d_loss: nan - val_classifier_loss: nan - val_classifier_accuracy: 0.6842\n",
      "Epoch 2/20\n",
      " 5/10 [==============>...............] - ETA: 1:32 - loss: nan - decoder_3d_loss: nan - decoder_2d_loss: nan - discriminator_3d_loss: nan - discriminator_2d_loss: nan - classifier_loss: nan - classifier_accuracy: 0.6000"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    [X_train_3d, X_train_2d],\n",
    "    {\n",
    "        'decoder_3d': X_train_3d,\n",
    "        'decoder_2d': X_train_2d,\n",
    "        'discriminator_3d': disc_targets_3d_train,\n",
    "        'discriminator_2d': disc_targets_2d_train,\n",
    "        'classifier': y_train\n",
    "    },\n",
    "    epochs=20,\n",
    "    batch_size=8,\n",
    "    validation_data=(\n",
    "        [X_test_3d, X_test_2d],\n",
    "        {\n",
    "            'decoder_3d': X_test_3d,\n",
    "            'decoder_2d': X_test_2d,\n",
    "            'discriminator_3d': disc_targets_3d_test,\n",
    "            'discriminator_2d': disc_targets_2d_test,\n",
    "            'classifier': y_test\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a84aa083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Test Accuracy: 68.42%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the classifier accuracy on the test set\n",
    "evaluation = model.evaluate(\n",
    "    [X_test_3d, X_test_2d],\n",
    "    {\n",
    "        'decoder_3d': X_test_3d,\n",
    "        'decoder_2d': X_test_2d,\n",
    "        'discriminator_3d': disc_targets_3d_test,\n",
    "        'discriminator_2d': disc_targets_2d_test,\n",
    "        'classifier': y_test\n",
    "    },\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Get the index of the classification loss and accuracy (depends on the order of outputs)\n",
    "classification_accuracy = evaluation[model.metrics_names.index('classifier_accuracy')]\n",
    "\n",
    "print(f'Classifier Test Accuracy: {classification_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2e39325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "devices = tf.config.list_physical_devices()\n",
    "print(devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ded408c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "73cd664e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tripti/miniconda3/lib/python3.9/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] Can't close file (file write failed: time = Thu Nov 28 18:09:47 2024\n, filename = 'models/fusion_model_1.h5', file descriptor = 84, errno = 28, error message = 'No space left on device', buf = 0xbfd3120, total write size = 47824, bytes this sub-write = 47824, bytes actually written = 18446744073709551615, offset = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/fusion_model_1.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/h5py/_hl/files.py:581\u001b[0m, in \u001b[0;36mFile.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid\u001b[38;5;241m.\u001b[39mvalid:\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;66;03m# We have to explicitly murder all open objects related to the file\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \n\u001b[1;32m    578\u001b[0m     \u001b[38;5;66;03m# Close file-resident objects first, then the files.\u001b[39;00m\n\u001b[1;32m    579\u001b[0m     \u001b[38;5;66;03m# Otherwise we get errors in MPI mode.\u001b[39;00m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid\u001b[38;5;241m.\u001b[39m_close_open_objects(h5f\u001b[38;5;241m.\u001b[39mOBJ_LOCAL \u001b[38;5;241m|\u001b[39m \u001b[38;5;241m~\u001b[39mh5f\u001b[38;5;241m.\u001b[39mOBJ_FILE)\n\u001b[0;32m--> 581\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid\u001b[38;5;241m.\u001b[39m_close_open_objects(h5f\u001b[38;5;241m.\u001b[39mOBJ_LOCAL \u001b[38;5;241m|\u001b[39m h5f\u001b[38;5;241m.\u001b[39mOBJ_FILE)\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    584\u001b[0m     _objects\u001b[38;5;241m.\u001b[39mnonlocal_close()\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:355\u001b[0m, in \u001b[0;36mh5py.h5f.FileID._close_open_objects\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] Can't close file (file write failed: time = Thu Nov 28 18:09:47 2024\n, filename = 'models/fusion_model_1.h5', file descriptor = 84, errno = 28, error message = 'No space left on device', buf = 0xbfd3120, total write size = 47824, bytes this sub-write = 47824, bytes actually written = 18446744073709551615, offset = 0)"
     ]
    }
   ],
   "source": [
    "model.save(\"models/fusion_model_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603237ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
